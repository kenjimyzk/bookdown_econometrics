---
output: html_document
---
# 操作変数法

```{r include=FALSE}
rm(list = ls())
library(knitr)
opts_chunk$set(echo = TRUE,error = FALSE,warning = FALSE,collapse = TRUE,cache = TRUE,cache.extra = rand_seed, autodep = TRUE)
```


```{r}
library(mosaic)
library(stargazer)
library(AER)
library(broom)
library(wooldridge)
set.seed(2000)
```


## 操作変数
```{r message=FALSE, warning=FALSE}
data("mroz", package="wooldridge")
df <- subset(mroz, inlf==1)
```

これまで回帰モデルで一致推定量を得るためには次の仮定が必要であった.

1. 母集団が線形モデル
2. 標本が無作為抽出
3. 誤差項が平均ゼロで説明変数と無相関
4. 説明変数に多重共線性が存在しない

3つ目の説明変数が必ずしも成立しない場合の推定方法を紹介する.

そのために, 外生変数と内生変数と操作変数の3つの概念を導入する.
説明変数を外生変数と内生変数に分ける.
誤差項と相関が無い説明変数を **外生変数** といい,
誤差項と相関がある説明変数を **内生変数** という.
**操作変数** とは, 説明変数に含まれず, 説明変数と相関をもち, 誤差項と相関をもたない変数のことである.
なお操作変数の個数は内生変数の個数より多いと仮定する.

R においては次のコマンドを実行すればよい.
ここで被説明変数は `log(wage)`,
内生変数は `educ`,
操作変数は `fatheduc` である.

```{r}
fm  <- ivreg(log(wage)~educ|fatheduc, data=df)
coef(fm)
```

傾きの推定値は以下でも実行可能である.
```{r}
with(df, cov(log(wage),fatheduc)/cov(educ,fatheduc))
```

## シミュレーション
つぎのシミュレーションを考える.
```{r}
simdata1 <- function (N=100) {
  z<-rnorm(N, mean =10, sd = 2)
  u <- rnorm(N)
  x <- z + u
  y<-1+x+u
  data.frame(x,y,z)
}
```

変数の分散ｎは以下である.
$$
V[Z]=4, V[U]=1, V[X]=V[Z]+V[U]=5
$$

共分散は以下である.
$$
Cov[X,U]=V[U]=1, Cov[X,Z]=V[Z]=4, Corr[X,Z]^2=4/5
$$


```{r, eval = FALSE}
# coef_iv <- function(data) {  
#   fm <- lm(y~x, data = data)
#   estimate <- coeftest(fm)[2,1]
#   se <- coeftest(fm)[2,2]
#   tval <- (estimate - 1)/se
#   fm_iv <- ivreg(y~x|z,data=data)
#   estimate_iv <- coeftest(fm_iv)[2,1]
#   se_iv <- coeftest(fm_iv)[2,2]
#   tval_iv <- (estimate_iv - 1)/se_iv
#   data.frame(estimate, se, tval,estimate_iv,se_iv,tval_iv)
# }
# siml_iv <- do(500) * coef_iv(simdata1())
```

```{r}
siml_iv <- do(500) * {simdata1() %>% {
  df_ols <- lm(y~x,data=.) %>% tidy() %>%
    filter(term == "x") %>% select(estimate, std.error) %>%
    mutate(tval = (estimate-1)/std.error)
  df_iv <- ivreg(y~x|z,data=.) %>% tidy() %>%
    filter(term == "x") %>% select(estimate, std.error) %>%
    rename(estimate_iv = estimate, std.error_iv = std.error) %>%
    mutate(tval_iv = (estimate_iv-1)/std.error_iv)
  bind_cols(df_ols,df_iv)
}}
```

```{r}
siml_iv %>% dplyr::select(estimate, estimate_iv) %>%
  tidyr::gather(variable,value) %>%
  favstats(~value|variable,data = .)
```

最小二乗法による推定値のヒストグラムは以下である.
```{r}
histogram(~estimate, data = siml_iv,xlim=c(0,2))
```

操作変数法による推定値のヒストグラムは以下である.
```{r}
histogram(~estimate_iv, data = siml_iv,xlim=c(0,2))
```

## 2段階最小二乗法
説明変数が複数あり, 操作変数の数が内生変数の数以上のとき, 係数の一致推定量を得るには二段階最小自乗法を用いる.
二段階最小二乗法は次の手順で実行される:

1. それぞれの内生変数を外生変数と操作変数に回帰させて, その予測値を得る.
2. 被説明変数を外生変数と内生変数の予測値に回帰させて, その係数を得る.

この係数が一致推定量になるための条件は以下である.

+ 母集団が線形モデル
+ 標本が無作為抽出
+ 誤差項が平均ゼロで操作変数と外生変数に対して独立.
+ 操作変数は内生変数と相関をもつ.
+ 外生変数と内生変数の予測値に多重共線性が存在しない`

R においては次のコマンドを実行すればよい.
ここで被説明変数は `log(wage)`,
内生変数は `educ`, 外生変数は `expr`, `I(expr^2)`,
操作変数は `motheduc`, `fatheduc` である.

```{r}
fm  <- ivreg(log(wage)~educ+exper+I(exper^2)|
            exper+I(exper^2)+motheduc+fatheduc,
            data=df)
summary(fm)
```

実際の二段階最小二乗法でも確認できる.
ただし標準誤差の値が異なっている.
なぜなら残差は内生変数および外生変数から算出させる必要があるが,
以下のやりかただと内生変数の予測値および外生変数から算出するためである.

```{r}
ols1 <- lm(educ~exper+I(exper^2)+motheduc+fatheduc, data = df)
ols2 <- lm(log(wage)~fitted(ols1)+exper+I(exper^2), data = df)
summary(ols2)
```

### 複数制約の検定

帰無仮説が複数制約のワルド検定は以下のように実施する.
例えば, 2つの外生変数の係数がゼロのときの仮説検定をRで実行するには以下を実施する.

```{r}
fm0 <- ivreg(log(wage)~educ|motheduc+fatheduc,data=df)
waldtest(fm0,fm)
```

もしくは
```{r}
linearHypothesis(fm, c("exper","I(exper^2)"))
```


LM検定は以下のように実施すればよい.
```{r}
# lmt <- lm(resid(fm0)~educ + exper + I(exper^2) ,data=df)
# (lmt <- nrow(df)*summary(lmt)$r.squared)
lmt <- lm(resid(fm0)~educ + exper + I(exper^2) ,data=df) %>%
  rsquared * nrow(df)
lmt
1-pchisq(lmt,df=3)
```

## 特定化検定
またいくつかの特定化検定も以下のコマンドで実施できる.
```{r}
summary(fm, diagnostics = TRUE)
```

### Weak instruments
操作変数が内生変数と弱い相関関係しかない場合, 弱操作変数という.
それぞれの内生変数に対して, 
帰無仮説を内生変数を外生変数のみ回帰させたモデルとし,
対立仮説を内生変数を外生変数および操作変数のみ回帰させたモデルとし, F検定を実施する.

```{r}
ols0 <- lm(educ ~ exper + I(exper^2), data = df)
waldtest(ols0, ols1)
```

### Du-Hausman 検定
Du-Hausman 検定は
帰無仮説が誤差項と説明変数が無相関, 対立仮説が誤差項と説明変数が相関ありの検定をおこなう.
帰無仮説のもと, OLSも2SLSも一致推定量である.
よって検定統計量のP値が十分小さいなら,
帰無仮説は棄却して, より効率的な最小二乗法を実施する.
そうでなければ操作変数法を選択する.

具体的には以下のF検定を実施する.

1. それぞれの内生変数を外生変数に回帰したときの残差をえる. (`resid(ols1)`)
2. 被説明変数を説明変数に回帰する (`ols3`)
3. 被説明変数を説明変数および先程の残差に回帰する (`ols4`)
4. これらの残差の係数はゼロであるという帰無仮説のもとF検定を実施する.

```{r}
ols3 <- lm(log(wage) ~ educ  + exper + I(exper^2), data = df)
ols4 <- update(ols3, . ~ . + resid(ols1))
waldtest(ols3,ols4)
```

### Sargan 検定
Sargan 検定は
誤差項が操作変数 (および外生変数) と相関しているかどうかを検定する.
帰無仮説が相関が無い場合で, 対立仮説は相関がある場合である.

LM検定を実施する.

1. 二段階最小二乗法を実施したときの残差をえる. (`resid(fm)`)
2. 残差を外生変数 (`exper`, `I(exper^2)`) および操作変数 (`motheduc`, `fatheduc`) に回帰する. 
3. 回帰の決定係数に観測数を乗じたLM統計量をえる.
4. 検定統計量は, 帰無仮説のもと, 操作変数の数 (2) から内生変数の数 (1) を差し引いた自由度のカイ二乗分布にしたがう.

```{r}
jt <- lm(resid(fm)~exper+I(exper^2)+motheduc+fatheduc,data=df) %>%
  rsquared() * nrow(df)
# (jt <- nrow(df)*summary(jt)$r.squared)
jt
1-pchisq(jt,df=1)
```

## ロバスト分散
以上の分析は, 誤差項が操作変数と独立の場合の分析である.
独立でない場合, 推定量の分散が変わりうる.
そうした分散をロバスト分散という.
ロバスト分散にもとづく, 推定結果は次のコマンドで実施する.
```{r}
summary(fm, vcov = vcovHC, df = Inf)
```

また次のコマンドで係数結果のみ出力可能である.
```{r}
coeftest(fm, vcov=vcovHC)
```

ロバスト分散のもとでの複数制約の検定は以下を実施する.
```{r}
waldtest(fm0,fm, vcov=vcovHC)
```


### 分散不均一の検定
誤差項が操作変数と独立なら条件付き分散は操作変数に無関係で均一である.
これを利用して分散均一を帰無仮説に, 分散不均一を対立仮説にしたBP検定が実行可能である.
ただ通常のコマンド `bptest` では正しく実行できないので注意が必要である.
内生変数は含まれないが, 外生変数のみならず操作変数も含めるかどうかは想定するモデルによる.

```{r}
bpt <- lm(I(resid(fm)^2)~exper + I(exper^2) +
            motheduc + fatheduc,data=df) %>%
  rsquared * nrow(df)
bpt
1-pchisq(bpt,df=4)
```


